{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 7: Spark Programming\n",
    "\n",
    "In what follows, you can find pyspark code for the examples we saw in class.\n",
    "Many of the examples follow examples found in [Learning Spark: Lightning-Fast Big Data Analysis, by Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia](http://www.amazon.com/Learning-Spark-Lightning-Fast-Data-Analysis-ebook/dp/B00SW0TY8O), which you can also find at Aalto's library.\n",
    "\n",
    "## Setup\n",
    "\n",
    "These instructions should work for Mac and Linux. We'll assume you'll be using python3.\n",
    "\n",
    "To run the following on your computer, make sure that pyspark is in your PYTHONPATH variable.\n",
    "You can do that by [downloading](https://spark.apache.org/downloads.html) a zipped file with Spark, extracting it into its own folder (e.g., `spark-1.6.0-bin-hadoop2.6/`) and then executing the following commands in bash.\n",
    "```\n",
    "export PYSPARK_PYTHON=python3\n",
    "export SPARK_HOME=/path/to/spark-1.6.0-bin-hadoop2.6/\n",
    "export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np # we'll be using numpy for some numeric operations\n",
    "sc = pyspark.SparkContext(master=\"local\", appName=\"tour\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = sc.textFile(\"myfile.txt\") # load data\n",
    "text.count() # count lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = sc.textFile(\"myfile.txt\") # load data\n",
    "\n",
    "# count only lines that mention \"Spark\"\n",
    "spark_lines = text.filter(lambda line: 'Spark' in line)\n",
    "spark_lines.count() # count lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lambda functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Lambda expressions](https://docs.python.org/3.5/howto/functional.html#small-functions-and-the-lambda-expression) are an easy way to write short functions in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = lambda line: 'Spark' in line\n",
    "f(\"we are learning Spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(line):\n",
    "    return 'Spark' in line\n",
    "f(\"we are learning Spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating RDDS\n",
    "\n",
    "We saw that we can create RDDs by loading files from disk. We can also create RDDs from Python collections or transforming other RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sc.parallelize([0,1,2,3,4,5,6,7,8,9]) # create RDD from Python collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_squared = data.map(lambda num: num ** 2) # transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of RDD operations in Spark: *transformations* and *actions*. Transfromations create new RDDs from other RDDs. Actions extract information from RDDs and return it to the driver program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize([0,1,2,3,4,5,6,7,8,9]) # creation of RDD\n",
    "data_squared = data.map(lambda num: num ** 2) # transformation\n",
    "data_squared.collect() # action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy evaluation\n",
    "\n",
    "RDDs are **evaluated lazily**. This means that Spark will not materialize an RDD until it has to perform an action. In the example below, `primesRDD` is not evaluated until action `collect()` is performed on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_prime(num):\n",
    "    \"\"\" return True if num is prime, False otherwise\"\"\"\n",
    "    if num < 1 or num % 1 != 0:\n",
    "        raise Exception(\"invalid argument\")\n",
    "    for d in range(2, int(np.sqrt(num) + 1)):\n",
    "        if num % d == 0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43]\n"
     ]
    }
   ],
   "source": [
    "numbersRDD = sc.parallelize(range(1, 1000000)) # creation of RDD\n",
    "primesRDD = numbersRDD.filter(is_prime) # transformation\n",
    "\n",
    "# primesRDD has not been materialized until this point\n",
    "\n",
    "primes = primesRDD.collect() # action\n",
    "print(primes[:15]) # this code does not involve Spark computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence\n",
    "\n",
    "RDDs are **ephemeral** by default, i.e. there is no guarantee they will remain in memory after they are materialized. If we want them to `persist` in memory, possibly to query them repeatedly or use them in multiple operations, we can ask Spark to do this by calling `persist()` on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 78499 prime numbers\n",
      "Here are some of them:\n",
      "[1, 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67]\n"
     ]
    }
   ],
   "source": [
    "primesRDD.persist() # we're asking Spark to keep this RDD in memory\n",
    "\n",
    "print(\"Found\", primesRDD.count(), \"prime numbers\") # first action -- causes primesRDD to be materialized\n",
    "print(\"Here are some of them:\")\n",
    "print(primesRDD.take(20)) # second action - RDD is already in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do not need `primesRDD` in memory anymore, we can tell Spark to discard it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[10] at collect at <ipython-input-10-d586bf285b26>:6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primesRDD.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long does it take to collect `primesRDD`? Let's time the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 10.2 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "primes = primesRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran the above on my laptop, it took about more than 10s. That's because Spark had to evaluate `primesRDD` before performing `collect` on it.\n",
    "\n",
    "How long would it take if `primesRDD` was already in memory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[10] at collect at <ipython-input-10-d586bf285b26>:6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primesRDD.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 273.88 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "1 loops, best of 3: 37.3 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "primes = primesRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran the above on my laptop, it took about 40ms to collect `primesRDD` - that's almost $300$ times faster compared to when the RDD had to be recomputed from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Passing functions\n",
    "\n",
    "When we pass a function as a parameter to an RDD operation, the function can be specified either as a lambda function or as a reference to a function defined elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize(range(10))\n",
    "squares = data.map(lambda x: x**2)\n",
    "squares.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    \"\"\" return the square of a number\"\"\"\n",
    "    return x**2\n",
    "\n",
    "data = sc.parallelize(range(10))\n",
    "squares = data.map(f)\n",
    "squares.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful, though: if the function that you pass as argument to an RDD operation \n",
    "* is an object method, or\n",
    "* references an object field,\n",
    "\n",
    "then Spark will ship the entire object to the cluster nodes along with the function.\n",
    "\n",
    "This is demonstrated in the piece of code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SearchFunctions(object):\n",
    "    def __init__(self, query):\n",
    "        self.query\n",
    "        \n",
    "    def is_match(self, s):\n",
    "        return self.query in s\n",
    "    \n",
    "    def get_matches_in_rdd_v1(self, rdd):\n",
    "        return rdd.filter(self.is_match) # the function is an object method\n",
    "    \n",
    "    def get_matches_in_rdd_v2(self, rdd):\n",
    "        return rdd.filter(lambda x: self.query in x) # the function references an object field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a better way to implement the two methods above (`get_matches_in_rdd_v1` and `get_matches_in_rdd_v2`), if we want to avoid sending a SearchFunctions object for computation to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SearchFunctions(object):\n",
    "    def __init__(self, query):\n",
    "        self.query\n",
    "        \n",
    "    def is_match(self, s):\n",
    "        return self.query in s\n",
    "    \n",
    "    def get_matches_in_rdd(self, rdd):\n",
    "        query = self.query\n",
    "        return rdd.filter(lambda x: query in x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## map and flatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', 'how', 'are', 'you', 'how', 'do', 'you', 'do']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases = sc.parallelize([\"hello world\", \"how are you\", \"how do you do\"])\n",
    "\n",
    "words = phrases.flatMap(lambda phrase: phrase.split(\" \"))\n",
    "\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', 'world'], ['how', 'are', 'you'], ['how', 'do', 'you', 'do']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases = sc.parallelize([\"hello world\", \"how are you\", \"how do you do\"])\n",
    "\n",
    "words = phrases.map(lambda phrase: phrase.split(\" \"))\n",
    "\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## (Pseudo) set operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[22] at parallelize at PythonRDD.scala:423"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneRDD = sc.parallelize([1, 1, 1, 2, 3, 3, 4, 4])\n",
    "oneRDD.persist()\n",
    "otherRDD = sc.parallelize([1, 4, 4, 7])\n",
    "otherRDD.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 2, 3, 3, 4, 4, 1, 4, 4, 7]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneRDD.union(otherRDD).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 3]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneRDD.subtract(otherRDD).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneRDD.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 1]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneRDD.intersection(otherRDD).collect() # removes duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (1, 4), (1, 4), (1, 7), (1, 1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneRDD.cartesian(otherRDD).collect()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize([1,43,62,23,52])\n",
    "data.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3188536"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.reduce(lambda x, y: x * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137823683725010149883130929"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.reduce(lambda x, y: x**2 + y**2) # this does NOT compute the sum of squares of RDD elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8927.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.reduce(lambda x, y: np.sqrt(x**2 + y**2)) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize([1,43,62,23,52])\n",
    "aggr = data.aggregate(zeroValue = (0,0),\n",
    "                      seqOp = (lambda x, y: (x[0] + y, x[1] + 1)),\n",
    "                      combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1])))\n",
    "aggr[0] / aggr[1] # average value of RDD elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('$APPL', 'x + y'), ('$AMZN', 'x + y'), ('$GOOG', 706.2)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD = sc.parallelize([ ('$APPL', 100.64), ('$GOOG', 706.2), ('$AMZN', 552.32), ('$APPL', 100.52), ('$AMZN', 552.32) ])\n",
    "\n",
    "pairRDD.reduceByKey(lambda x, y: \"x + y\").collect() # sum of values per key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## combineByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('$APPL', 100.58), ('$AMZN', 552.32), ('$GOOG', 706.2)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD = sc.parallelize([ ('$APPL', 100.64), ('$GOOG', 706.2), ('$AMZN', 552.32), ('$APPL', 100.52), ('$AMZN', 552.32) ])\n",
    "\n",
    "aggr = pairRDD.combineByKey(createCombiner = lambda x: (x, 1),\n",
    "                           mergeValue = lambda x, y: (x[0] + y, x[1] + 1),\n",
    "                           mergeCombiners = lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "aggr.map(lambda x: (x[0], x[1][0]/x[1][1])).collect() # average value per key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## (inner) join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tuukka', (10, 10)), ('Leena', (9, 10))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_a = sc.parallelize([ (\"Antti\", 8), (\"Tuukka\", 10), (\"Leena\", 9)])\n",
    "course_b = sc.parallelize([ (\"Leena\", 10), (\"Tuukka\", 10)])\n",
    "\n",
    "result = course_a.join(course_b)\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Accumulators\n",
    "\n",
    "This example demonstrates how to use accumulators.\n",
    "The map operations creates an RDD that contains the length of lines in thetext file - and while the RDD is materialized, an accumulator keeps track of how many lines are long (longer than $30$ characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = sc.textFile(\"myfile.txt\")\n",
    "long_lines = sc.accumulator(0) # create accumulator\n",
    "\n",
    "def line_len(line):\n",
    "    global long_lines # to reference an accumulator, declare it as global variable\n",
    "    length = len(line)\n",
    "    if length > 30:\n",
    "        long_lines += 1 # update the accumulator\n",
    "    return length\n",
    "\n",
    "llengthRDD = text.map(line_len)\n",
    "llengthRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_lines.value # this is how we obtain the value of the accumulator in the driver program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning\n",
    "\n",
    "In the example above, we update the value of an accumulator within a transformation (map). This is **not recommended**, unless for debugging purposes! The reason is that, if there are failures during the materialization of `llengthRDD`, some of its partitions will be re-computed, possibly causing the accumulator to double-count some the the long lines.\n",
    "\n",
    "It is advisable to use accumulators within actions - and particularly with the `foreach` action, as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = sc.textFile(\"myfile.txt\")\n",
    "long_lines = sc.accumulator(0)\n",
    "\n",
    "def line_len(line):\n",
    "    global long_lines\n",
    "    length = len(line)\n",
    "    if length > 30:\n",
    "        long_lines += 1\n",
    "\n",
    "text.foreach(line_len)\n",
    "long_lines.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast variable\n",
    "\n",
    "We use *broadcast variables* when many operations depend on the same large static object - e.g., a large lookup table that does not change but provides information for other operations. In such cases, we can make a broadcast variable out of the object and thus make sure that the object will be shipped to the cluster only once - and not for each of the operations we'll be using it for.\n",
    "\n",
    "The example below demonstrates the usage of broadcast variables. In this case, we make a broadcast variable out of a dictionary that represents an address table. The tablke is shipped to cluster nodes only once across multiple operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Anwar': 'T, B103', 'Darshan': 'T, A325', 'Orestis': 'T, A341', 'Michael': 'OIH, B253.2'}\n",
      "{'Karmen': 'VTT, 74', 'Michael': 'OIH, B253.2', 'Anu': 'Chem. A143'}\n"
     ]
    }
   ],
   "source": [
    "def load_address_table():\n",
    "    return {\"Anu\": \"Chem. A143\", \"Karmen\": \"VTT, 74\", \"Michael\": \"OIH, B253.2\",\n",
    "            \"Anwar\": \"T, B103\", \"Orestis\": \"T, A341\", \"Darshan\": \"T, A325\"}\n",
    "\n",
    "address_table = sc.broadcast(load_address_table())\n",
    "\n",
    "def find_address(name):\n",
    "    res = None\n",
    "    if name in address_table.value:\n",
    "        res = address_table.value[name]\n",
    "    return res\n",
    "\n",
    "people = sc.parallelize([\"Anwar\", \"Michael\", \"Orestis\", \"Darshan\"])\n",
    "pairRDD = people.map(lambda name: (name, find_address(name))) # first operation that uses the address table\n",
    "print(pairRDD.collectAsMap())\n",
    "\n",
    "other_people = sc.parallelize([\"Karmen\", \"Michael\", \"Anu\"])\n",
    "pairRDD = other_people.map(lambda name: (name, find_address(name))) # second operation that uses the address table\n",
    "print(pairRDD.collectAsMap())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping\n",
    "\n",
    "Call `stop()` on the SparkContext object to shut it down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
