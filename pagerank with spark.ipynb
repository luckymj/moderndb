{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np # we'll be using numpy for some numeric operations\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_dataset(N, degree):\n",
    "    # the returned dataset is a list\n",
    "    # each element of this list is a tuple,\n",
    "    # representing one page and its links (to other pages)\n",
    "    # (each page is represented by an integer)\n",
    "    dataset = []\n",
    "    for i in range(N):\n",
    "        dataset.append((i, list(range(i+1, min(N, i+degree+1)))))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 10000 # number of pages, each represented with a number\n",
    "degree = 10 # degree of each page (constant for all pages)\n",
    "\n",
    "# make an RDD\n",
    "linksRDD = sc.parallelize((i, list(range(i+1, min(N, i+degree+1)))) for i in range(N))\n",
    "\n",
    "# ask spark to keep it in memory\n",
    "linksRDD.persist()\n",
    "\n",
    "# action\n",
    "N = linksRDD.count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize rank scores of pages\n",
    "ranks = linksRDD.map(lambda x: (x[0], 1/N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## constrants and auxilliary functions\n",
    "\n",
    "ITERATIONS = 10; alpha = 0.15\n",
    "\n",
    "def contr(x):\n",
    "    \"\"\"\n",
    "    return the contributions of a single page u\n",
    "    \"\"\"\n",
    "    page_u, _tmp = x    # x is a pair\n",
    "    links, rank = _tmp  # _tmp is a pair\n",
    "\n",
    "    result = []  # the result is a list of pairs\n",
    "    for node_v in links:\n",
    "        result.append((node_v, rank / len(links)))\n",
    "    return result\n",
    "\n",
    "def smoothen(x):\n",
    "    \"\"\"\n",
    "    calculate a pagerank score for node v\n",
    "    from the sum of contributions it receives\n",
    "    from other nodes\n",
    "    \"\"\"\n",
    "    node_v = x[0]  # x is a pair\n",
    "    sum_of_contributions = x[1]\n",
    "    return (node_v, alpha / N + (1 - alpha)* sum_of_contributions)\n",
    "\n",
    "def add(x, y):\n",
    "    \"\"\" return the sum of x and y\"\"\"\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pagerank computation\n",
    "for i in range(ITERATIONS):\n",
    "    contribs = linksRDD.join(ranks).flatMap(contr)\n",
    "    ranks = contribs.reduceByKey(add).map(smoothen) # no action yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ranks.collect() # action - collect result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
